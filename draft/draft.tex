\documentclass{article}

\usepackage{geometry}
\usepackage{graphicx}
\usepackage[style=authoryear]{biblatex}
\usepackage{booktabs}

% Double space
\usepackage{setspace}
\doublespacing

% Read in bibliography
\addbibresource{movies_bib.bib}

\author{Gideon Moore}
\title{Industrial Organization III: \\ Final Paper}
\date{June 9, 2024}

\begin{document}

\maketitle 

\begin{abstract}
    I propose a novel method for estimating competition between products using text embeddings. Applying this method to the movie industry, I estimate impacts of competition which are reasonable in both sign and magnitude. Facing additional competitors lowers a film's revenue, and more similar competitors are more damaging. I find that if a movie could replace a competitor in the 90th percentile of similarity with one in the 10th percentile of similarity, its revenue would increase by 0.32\% to 0.44\%.
\end{abstract}

\section{Introduction}

Demand estimation is a central pillar of the economist toolkit, informing antitrust regulation, optimal taxation, and trade policy. However, demand estimation is often limited to markets with easily quantifiable characteristics, such as the storage in the market for hard drives or wattage in the market for light bulbs. We should not ignore markets with difficult-to-quantify characteristics simply because they are difficult to quantify; though it is hard to put a number how funny a movie is, we understand that a firm monopolizing comedy films is likely to be bad for consumers. Thus, it is important economists develop tools to estimate demand in markets for goods with difficult-to-quantify characteristics.

I propose a new method for quantifying product characteristics using text descriptions. By embedding text descriptions of a product in characteristic space, we as econometricians can identify which products are most similar. Thus, we can predict which products compete most closely with others and estimate how the presence of one product impacts the sales of another.

While this method has broad applicability, I use the film industry as a case study. Movies are a natural laboratory for several reasons. First, movies have a broad consumer base, such that readers have an intuitive sense whether my method ``makes sense''--even if you haven't seen \emph{Spider-Man}, you likely have a sense whether it is more similar to \emph{Man of Steel} or \emph{The Notebook}, and can use this lens to judge my results. Second, movie characteristics are often amorphous: while we can easily measure the wattage of a lightbulb, it is much harder to put a number on how funny or violent a movie is--and harder still given the lack of clean data. Finally, the movie industry has specific market characteristics which make demand estimation straightforward conditional on my method. Uniform pricing shuts down the endogeneous pricing channel of competition, and long production pipelines force studios to choose characteristics without knowing anything about their competitors.

Using data from BoxOfficeGuru and TheMovieDB, I estimate the impact of competitor characteristics on film revenue for each weekend from 1997-2019. I find substitution patterns which are plausible in both magnitude and direction, and which are robust to the choice of distance metric. As a movie becomes more similar to its competitor, it becomes more and more detrimental to the competitor's revenue. I find that replacing a competitor in the 90th percentile of similarity with one in the 10th percentile of similarity would increase revenue by 0.32\% to 0.44\%.

\section{Literature}

This paper contributes to three strands of literature. First, I implement a novel use for text embeddings, innovating on the text-as-data literature in economics. Second, given my demand estimation method is based on position in latent characteristic space, I am also closely connected to existing work on spatial competition. Finally, given my film case study, I also contribute to the literature on the economics of the film industry. 

\subsection{Text as Data}

This paper's primary contribution is to the literature on text as data, well summarized in \textcite{gentzkow2019EL}. The statistical analysis of text has a long history; the earliest example I have found is \textcite{mendenhall1887S} using word frequency to predict the author of a mysterious text. This is an early example of \emph{bag of words} methods, which treat text as a collection of words rather than a structured object. Other types of bag of words methods include term frequency-inverse document frequency (TF-IDF), which downweights words that appear in many documents, and Latent Dirichlet Allocation (LDA), which uses a generative model to infer topics from word concurrences.

I would argue these models work well for long documents such as patents and books, but struggle when working with (what I will call) ``terse'' texts. If one paragraph includes the word ``wizardry'' while the other includes the word ``sorcery,'' a bag of words model will not capture the similarity between the two paragraphs. In large enough documents, there is some idea that the two words are likely to appear in similar contexts, but this is difficult to rely on when the sample text is only one or two sentences. 

For terse contexts such as mine, I prefer using \emph{text embedding methods} as pioneered by Google's word2vec architecture \parencite{mikolov2013}. Rather than counting individual words, word2vec identifies similar words using co-occurances in a training sample, then maps these words to a lower-dimensional space. This allows for the identification of similar texts even when they share few or no words, since it is able to identify that synonyms like ``wizardry'' and ``sorcery'' map to the same concept.

word2vec, while an improvement on bag of words, still overlooks \emph{interactions} between words. While ``climate'' and ``change'' have meanings of their own, ``climate change'' is a largely distinct phenomenon. This is the motivation behind ``transformer'' architectures, pioneered again by Google in 2019's BERT sentence embeddings \parencite{devlin2019}. However, BERT limits itself to processing one sentence at a time and aggregating; thus, I prefer OpenAI's recent GPT-3 embeddings which can process entire paragraphs.

This is an improvement upon much of the economics literature, which still relies on bag of words methods. For example, \textcite{baker2016QJE} uses counts of ``uncertain'' words to estimate economic policy uncertainty. Similarly, \textcite{ederer2022} uses counts of word frequences to construct a vector used as a proxy for product characteristics and estimate cross-price elasticities.

The most similar paper to mine is \textcite{compiani2023} which uses text characteristics to estimate demand for technology products on Amazon. My paper innovates on theirs in multiple important ways. First, I have a much broader market; rather than focusing on fewer than 3,000 purchases in a specific Amazon retailer, I examine the entire United States movie market for multiple decades. Second, assumptions for demand estimation are much more plausible in the movie industry than in consumer electronics: the Kindle 8 is clearly designed endogeneously to the characteristics of the Kindle 7, whereas movies have long production pipelines which lock in product charactertics before competitor characteristics are known. Third, I use more advanced text embedding methods, pushing past word frequency and BERT-based methods to OpenAI's GPT-3 embeddings. Fourth, I (will...) benchmark my substitution patterns against ``real world'' user preference data from the MovieLens data, confirming my estimates capture actual substitution patterns. Finally, I estimate substitution patterns much more flexibly, allowing substitution to vary non-linearly and non-monotonically with distance between two products.

\subsection{Spatial Competition}

Though this paper does not focus on \emph{physical space}, I also contribute to the literature on spatial competition in the tradition of \textcite{hotelling1929E} and \textcite{salop1979BE}, where firms interact based on relative position in space. By projecting descriptions into characteristic space, I model high-dimensional competition between products. 

My closest predecessor studying competition in embedding space is \textcite{magnolfi2022} (hereafter MMS), who use t-Stochastic Triplet Embeddings to estimate competition in the cereal industry. I take heavy inspiration from their model, however substitute frontier text embedding methods from OpenAI for their triplet-based embeddings. Both they and I follow \textcite{pinkse2002E}, who first estimate product cross-elasticities as a function of distance in characteristic space.

\subsection{Movies}

The final strand of literature where I wish to highlight my contribution is on the economics of the film industry. 

Though I do not contribute to it, I benefit greatly from research documenting the uniform pricing puzzle \parencite{orbach2007IRLE, gil2009MS, ho2018MR}. This literature extensively documents movie theaters' use of uniform pricing across heterogeneous movies. While each paper suggests potential explanations for this phenomenon, my findings are agnostic to the true cause of uniform pricing; I simply use it as a convenient assumption to estimate demand.

Much of my understanding of the economics of film literature comes from a survey by \textcite{mckenzie2012ES}. I contribute to the ongoing literature founded by \textcite{prag1994CE} of determinents of box office demand. The authors find well-rated and well-advertised movies generally sell more tickets, and conditional on those two factors other observables are generally insignificant. \textcite{elberse2003MS} model both the supply and the demand for movies, finding that theatres' decision to screen a movie at all is an important factor for revenue. \textcite{devany1996E, devany1997EI, devany1999JCE, devany2004EDC} examine many factors of box office revenue, including the impact of word-of-mouth, the hazard rate of movie retirement, and estimating the film production function. \textcite{ravid2004B} study the impact of film \emph{content} on performance, finding that violence increases film profitability while sexual content does not. 

To the best of my knowledge, I am the first paper to examine the \emph{interplay} between competing films: how much is my revenue impacted by the existence of box office competitors?


\section{The Movie Industry}

In the tradition of industrial organization, I use an industry case study of the film industry to highlight the value of my method. Text characteristics are distinctively important for the movie industry because movie characteristics are otherwise difficult to quantify; for example, both \emph{The Dark Knight} and \emph{Ant-Man} are big-budget superhero action movies, but even a quick glance at the movie descriptions makes clear they are not substitutable. This is particularly important given movies are an ``experience good''--people rely on the description to inform their purchasing decision, as most people will not form opinions through repeat purchases. Recognizing this, we expect firms put more effort into the text descriptions of their movies than comparable firms might in more traditional goods markets, as these decisions are distinctively important to customer decisionmaking. Finally, the market for movies has two distinctive features which make it a strong candidate for demand estimation: long production pipelines and a tradition of uniform pricing.

\subsection{Production Timelines}

When estimating the impact of competitors' characteristics on demand for a movie, its characteristics must be exogeneous to the characteristics of its competitors. If characteristics are chosen as equilibrium objects, then the coefficients on proximity are no longer causal: the impact of locating near a close competitor becomes confounded with latent demand characteristics driving the \emph{choice} to locate near a close competitor. 

Fortunately, movie production timelines are both long and secretive. Since studios must purchase a script, hire a cast, and shoot all \emph{before} promoting a movie, it is difficult for a film to change its characteristics in response to its competitors' characteristics. I posit the existence of ``twin movies'' as evidence of this characteristic inelasticity. A pair of ``twin movies'' are two movies released in close proximity with very similar characteristics; the most famous examples, \emph{A Bug's Life} and \emph{Antz}, both released in fall of 1998 and feature early CGI animation of ants rebelling against oppression.

Twin movies are a persistent feature of the film industry: the Wikipedia page for the phenomenon lists nearly 300 pairs of twin movies. Studios are generally inclined to avoid releasing a twin movie due to fear of excess competition. In 2016, French director Xavier Giannoli released his film \emph{Marguerite}, inspired by the true story of a New York Socialite turned failed opera singer, in the same year as 20th Century Fox's \emph{Florence Foster Jenkins} retelling the same story. Giannoli told the \emph{Independent}: ``For me, it was terrible...I work a lot as a writer to find completely original stories. I don't want the audience to have the feeling, `oh, I saw that!''' \parencite{mottram2016I}.

Given filmmakers' expressed distaste for twin movies, why do they continue to exist? It suggests that studios are unable to change their movies' characteristics in response to their competitors' characteristics; at the point I learn of \emph{A Bug's Life}, it is too late for me to adjust \emph{Antz} to be more distinct. Thus, our demand estimation coefficients are identified--they do not capture the strategic interplay of positioning and substitutability, but instead the direct impact of competition on film sales.

This should produce testable hypotheses: conditional on the characteristics of a movie, the characteristics of its competitors should ``look like'' the characteristics of movies releasing in other weekends. One complication to this is we think movies have seasonality, so we can't simply compare movies within the same year in different months. Thus, we'd likely want to compare movies released in July of one specific year against movies released in July of \emph{other} years. This will work less well if movie content shifts significantly over time (for example, if we think there are more superhero movies in the 2010s than in the 2000s). 

One threat to this strategy would be if firms lack flexibility on their films' \emph{content}, but can move the film's release date \emph{forward or backward in time}. If I discover I am releasing my children's movie opposite \emph{Frozen}, I may wish to postpone my release date. This would threaten the assumption of exogeneous characteristics among competitors. I think this is unlikely to be a major concern for a couple of reasons. First, there are certain prime release dates which are likely to be more profitable than others; for example, releasing a family movie in November is much better than doing so in January, as you can capture the holiday season. Thus, if a movie targeted a holiday release, moving its release date to avoid competition would potentially be quite costly since there no longer exists consumer demand. Second, the continued existence of twin movies further suggests this is not a margin firms seem to adjust significantly on: if studios could easily move release dates to avoid competition, we would expect to see fewer twin movies than we do. 

\subsection{Uniform Pricing}

An ongoing puzzle within the film industry is the existence of a uniform pricing standard: within a timeslot, cinemas generally charge the same price for all movies regardless of excess demand. As a salient example, ``opening night'' showings of movies often sell out, yet theaters do not raise prices. Similarly, movies late in their run often have significant excess capacity, yet theathers do not lower prices.

There are many explanations for this phenomenon; I am agnostic about which of these is the true explanation. What is important is that ticket price \emph{is} uniform across films. This pricing anomaly supports two simplifying assumptions which do not hold water in most other settings.

First, we know \emph{movies do not compete on price.} Under more flexible pricing, we might believe two movies in close priximity would lower prices to compete for the same audience, raising the quantity sold overall. However, since prices are fixed we know that consumer decisions are driven entirely by product characteristics and idiosyncratic taste.

Second, we can \emph{use revenue as a proxy for quantity.} As described in \textcite{bond2021JME}, this assumption is often perilous since a variable markup means that revenue may not reflect true production. However, since prices are fixed, we can understand revenue as simply the quantity of tickets sold multiplied by some scalar price. Thus, even without observing quantity, I am confident that observed revenue is a close substitute. 


\section{Model} 

\subsection{Log-Linear Demand}

Following MMS, I estimate a log-linear demand model for movies. I start by including fixed effects for each movie and weekend. The distance between products $i$ and $j$ is given by some metric $d_{ij}$; a competitor's impact is given by via a flexible function of distance $f(d_{ij})$. The sum of individual competitors' impacts yields the total impact of competition on a given movie. Again following MMS, I estimate $f$ as a cubic polynomial in distance.

The first benefit of the cubic polynomial functional form here is that this assumption is falsifiable; if $f$ is not monotonically attenuating, our spatial competition model is likely misspecified. The second benefit of this functional form is it is estimable linearly; by distributing the grand sum over each individual term and factoring out the $\gamma$ coefficients, demand becomes a simple linear equation in $d_{ij}$, $d_{ij}^2$, and $d_{ij}^3$. Thus, traditional matrix inversion techniques remain effective. The final benefit is that the cubic polynomial is flexible; it imposes no structure on sign, derivative, or magnitude. Thus, I can capture potentially quite rich substitution patterns, as well as evaluate my model fit against ``reasonable'' priors. 

Unlike cereals, demand for movies is heavily based on \emph{novelty}; that is, a movie will generally sell more the week of release than two months later. Thus, I include a fixed effect $\lambda_{t - r(j)}$ to control for time since release, where $r(j)$ is the release date of movie $j$. 

The final model of demand for movie $j$ in time $t$ is as follows:

$$\ln(q_{jt}) = \alpha_j + \alpha_t + \sum_{k \neq j} f(d_{jk}; \gamma) + \lambda_{t - r(j)} + \varepsilon_{jt}$$

$$f(d_{jk}; \gamma) = \gamma_0 + \gamma_1 d_{jk} + \gamma_2 d_{jk}^2 + \gamma_3 d_{jk^k}$$

In the above specification, I purposely do not include price on the right hand side due to the uniform pricing assumption discussed above. We can interpret $f(d_{jk}; \gamma)$ as the cross-elasticity between movies $j$ and $k$; as movie $k$ becomes closer or further from movie $j$, $f$ maps how demand for movie $j$ changes. Thus, we generally expect to see $f$ monotonically attenuating in distance; as two movies become more distant, they should become increasingly insulated from each other.


\subsection{Age Interaction and Estimation}

The above model leaves something to be desired if we think \emph{new movies are more relevant for competitors as well}. For example, my Science Fiction movie is much more unhappy to release the same weekend as \emph{Star Wars} than to release two months after \emph{Star Wars}. To capture this, I need to account for the age of the competitors as well by scaling $f$ by some factor $\delta_{t - r(k)}$. To preserve power, I should topcode $\delta$ at some reasonable value based on what I see in the data; e.g., I may group movies with 10+ weeks. I can inform my topcoding threshold by looking for plateaus in the empirical distribution of sales as movies age.

In this case, our new model is:

$$\ln(q_{jt}) = \alpha_i + \alpha_t + \sum_{k \neq j} \delta_{t - r(k)} f(d_{jk}; \gamma) + \lambda_{t - r(j)} + \varepsilon_{jt}$$

Note this model is no longer linearly estimable, as $\delta_{t - r(k)}$ interacts multiplicatively with the $\gamma$ coefficients in $f$. I could manually pull out the fixed effects, then optimize over the possible values of $\delta$, $\lambda$, and $\gamma$, however I believe I have a more clever solution.

Note for any fixed $\delta$ vector, we know the model is linear in $\gamma$ and $\lambda$, which makes it easy to identify the least-squares solution using matrix inversion. Thus, I can perform a two-step procedure to identify the least-squares coefficients of my nonlinear model.

First, I choose a $\delta$. Then, using that $\delta$, I compute the least squares solution for $\gamma$ and $\lambda$. This gives me a sum of squared errors for this set of coefficients. I then use this sum of squared errors as the minimization objective to optimize over \emph{$\delta$ alone}. Since OLS always minimizes least squares \emph{within} a $\delta$ estimate, the optimal model must be the one which minimizes this quantity \emph{across} $\delta$s. The psuedocode for this algorithm is something like:

\begin{verbatim}
def fit_model(delta, data):
    for t in data:
        weekend = data[t]
        for j in weekend[movies]:
            gamma_0_val = 0
            gamma_1_val = 0
            gamma_2_val = 0
            gamma_3_val = 0
            for k != j in weekend[movies]:
                gamma_0_val += delta[movies[k][age]]
                gamma_1_val += delta[movies[k][age]] * distance(j, k)
                gamma_2_val += delta[movies[k][age]] * distance(j, k)^2
                gamma_3_val += delta[movies[k][age]] * distance(j, k)^3

    model = Panel(
        y = ln(data[revenue]),
        x = [
            gamma_0_val,
            gamma_1_val,
            gamma_2_val,
            gamma_3_val,
            lambda[data[age]],
        ],
        fe = [data[movie], data[weekend]]
    )

    return model

best_delta = minimize(
    lambda d: fit_model(d).sse, 
    data, 
    np.zeros(LEN_DELTA)
).x

best_model = fit_model(best_delta)
\end{verbatim}

I have not yet estimated this model, but I believe this procedure should be consistent. Unfortunately analytic standard errors likely won't work, but I can always bootstrap them. 

\section{Data}

\subsection{BoxOfficeGuru}

I scrape weekend box office receipts from 1997-2024 from BoxOfficeGuru.com, a website maintained by Gitesh Pandya. Pandya is a film consultant specializing in releasing Indian films for the North American market. For each weekend, the website lists the top 10-20 movies by North American box office revenue. I have nearly the complete set of weekends over this 27 year span; however, when Gitesh in on vacation (a couple weekends a year) the website is not updated. An example of the data is visible in figure \ref{fig:guru}.

\begin{figure}
    \includegraphics[width=\textwidth]{images/guru.jpg}
    \caption{BoxOfficeGuru Page for May 17-29, 2019}
    \label{fig:guru}
\end{figure}


\subsection{TheMovieDB}

I retrieve movie characteristics from the API of TheMovieDB, a user-generated database of movies. Specifically, I clean movie names and years retrieved from BoxOfficeGuru and query the search API for the top result released in the relevant year. For each movie, I can thus retrieve the description, genres, and original release language. Example movie descriptions are visible in table \ref{tab:tmdb_desc}.

\begin{table}
    \begin{tabular}{lp{11cm}}
        \toprule 
        \textbf{Movie} & \textbf{Description} \\
        \midrule
        \emph{Frozen} & Young princess Anna of Arendelle dreams about finding true love at her sister Elsaâ€™s coronation. Fate takes her on a dangerous journey in an attempt to end the eternal winter that has fallen over the kingdom. She's accompanied by ice delivery man Kristoff, his reindeer Sven, and snowman Olaf. On an adventure where she will find out what friendship, courage, family, and true love really means. \\
        \emph{The Notebook} & An epic love story centered around an older man who reads aloud to a woman with Alzheimer's. From a faded notebook, the old man's words bring to life the story about a couple who is separated by World War II, and is then passionately reunited, seven years later, after they have taken different paths. \\
        \emph{Avengers: Endgame} & After the devastating events of Avengers: Infinity War, the universe is in ruins due to the efforts of the Mad Titan, Thanos. With the help of remaining allies, the Avengers must assemble once more in order to undo Thanos' actions and restore order to the universe once and for all, no matter what consequences may be in store. \\
        \emph{A Bug's Life} & On behalf of ``oppressed bugs everywhere," an inventive ant named Flik hires a troupe of warrior bugs to defend his bustling colony from a horde of freeloading grasshoppers led by the evil-minded Hopper. \\
        \bottomrule
    \end{tabular} 
    \caption{Example Movie Descriptions from TheMovieDB}
    \label{tab:tmdb_desc}
\end{table}

\subsection{Sample Selection}

To avoid the impact of the COVID-19 pandemic, I restrict my sample to movies released in 2019 or before. Similarly, I limit only to movies originally released in English to avoid both non-English movie descriptions and differential sales trends among non-English speaking consumers.

Though I cannot find documentation for their methodology, genres on TMDB appear to be ordered; that is, the ``primary'' genre of a movie is listed first on the page. This is evident based on genre ordering reversals; for example, \emph{Godzilla x Kong: The New Empire} (a recent Kaiju movie) lists ``action'' before ``adventure;'' on the other hand, \emph{Kingdom of the Planet of the Apes} lists ``adventure'' before ``action.'' Thus, we know the ordering of genres is idiosyncratic by movie. When I must assign a genre to a movie, I use the first genre listed on TMDB.  

\section{Text Embeddings}

I use OpenAI's embedding model \texttt{text-embedding-3-small} API to embed movie descriptions. This produces a 1,536-dimensional vector for each movie description. The OpenAI embedding procedure trains a transformer to predict adjacent texts based on a large corpus, such that vectors with similar cosine similarity are likely to have similar next words \parencite{neelakantan2022ao, kusupati2022ao}.

I prefer the OpenAI embeddings to other text-as-data methods for a few reasons. First, movie descriptions are quite terse; most are around a paragraph, with a few as short as one or two sentences. Thus, traditional bag-of-words methods are likely to suffer, as word or bigram counts would be remarkably sparse. Because the OpenAI embedding model is trained on an enormous corpus of text, it can infer when descriptions are similar even when they share \emph{no} words--for example, a movie about ``wizardry'' would be similar to one about ``sorcery,'' which would not be true under a bag-of-words model.

Secondly, BERT embeddings tend to function at the sentence level (and word2vec embeddings function only at the word level!), while the OpenAI embeddings are generally more context-aware. Given the movie descriptions tend to be longer than a single sentence, I believe the OpenAI embeddings are likely to be more able to capture the full context of the movie description. Even so, I should likely also perform my procedure using BERT embeddings in my robustness checks.

In the long run I will likely want to use \texttt{text-embedding-3-large} for better performance. For exploratory purposes, I use the smaller model due to cost; the large model costs roughly 10x as much as the small model, though produces a 3,072-dimensional embedding.

As a check for the quality of the embeddings, I perform t-Stochastic Neighbor Embedding (t-SNE) on the embeddings to reduce them to two dimensions and highlight clustering. I find that the embeddings do indeed cluster by genre, as visible in figure \ref{fig:tsne}. Generally, we see action and adventure movies grouped in the top-right quadrant, science fiction and fantasy in the bottom-right quadrant, and romance and comedies on the left; that is, it seems as through the embeddings are not just clustering \emph{within} a genre, but also grouping \emph{similar genres}. 

\begin{figure}
    \includegraphics[width=\textwidth]{../tmdb/plots/tsne_genres.png}
    \caption{t-SNE of OpenAI Embeddings by Genre}
    \label{fig:tsne}
\end{figure}


\subsection{Similarity Measures}

I consider two methods of measuring embedding similarity. My first method is the most similar to MMS: I reduce the dimensionality of the 1,536-dimensional embedding to 6 dimensions using principal component analysis. From there, I can compute Euclidean distance between the PCA-reduced embeddings as a measure of similarity. 

While this is most similar to the MMS approach, my preferred specification uses cosine similarity of the full embeddings. This method uses all information contained in each embedding, rather than just the first six principal components. Moreover, OpenAI suggests the use of cosine similarity to compare embeddings in their documentation. 


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
    \begin{center}
        \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Baseline Movie}    & \textbf{Nearest Neighbor \#1}       & \textbf{Nearest Neighbor \#2}              \\ \midrule
        \textit{Frozen}            & \textit{The Little Mermaid}         & \textit{The Princess Diaries 2}            \\
        \textit{The Notebook}      & \textit{Collateral Beauty}          & \textit{I Still Believe}                   \\
        \textit{Avengers: Endgame} & \textit{X-MEN: Days of Future Past} & \textit{Guardians of the Galaxy, Volume 3} \\
        \textit{A Bug's Life}      & \textit{The Spongebob Movie}        & \textit{DC League of Super Pets}           \\ \bottomrule
        \end{tabular}
        \caption{Nearest Neighbors to Baseline Movies by Euclidean Distance}
        \label{tab:euclidean}
    \end{center}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
    \begin{center}
        \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Baseline Movie}    & \textbf{Nearest Neighbor \#1}   & \textbf{Nearest Neighbor \#2}       \\ \midrule
        \textit{Frozen}            & \textit{Frozen II}              & \textit{Mirror Mirror}              \\
        \textit{The Notebook}      & \textit{Message in a Bottle}    & \textit{The Longest Ride}           \\
        \textit{Avengers: Endgame} & \textit{Avengers: Infinity War} & \textit{Captain America: Civil War} \\
        \textit{A Bug's Life}      & \textit{The Ant Bully}          & \textit{Antz}                       \\ \bottomrule
        \end{tabular}
        \caption{Nearest Neighbors to Baseline Movies by Cosine Similarity}
        \label{tab:cosine}
    \end{center}
\end{table}

Nearest neighbors to some common movies under Euclidean and cosine distance are visible in tables \ref{tab:euclidean} and \ref{tab:cosine}, respectively. Euclidean distance appears to be capturing ``something''--the closest movies to \emph{Frozen} are both princess movies, and the closest movies to \emph{A Bug's Life} are both animated children's movies. However, I find the cosine-based neighbors to be more compelling: the closest movies to \emph{The Notebook} are other Nicholas Sparks book adaptations, and the closest movies to \emph{A Bug's Life} are \emph{The Ant Bully} and \emph{Antz}. Moreover, the closest neighbors to \emph{Frozen} and \emph{Avengers: Endgame} are in the same franchise: \emph{Frozen II} and \emph{Avengers: Infinity War} respectively. This improvement in neighbor quality leads me to prefer cosine similarity for my analysis.

\section{Elasticity Results}

First I run the analysis using Euclidean distance in six-dimensional space, following MMS. The regression results are visible in table \ref{tab:results_euclid}. The joint regression is highly significant, though the individual coefficients are iffier; given only the coefficient on $\gamma_0$ is significant, it may not be \emph{distance} that matters in this specification, but instead just the \emph{number of competitors}. However, it is also possible the other coefficients are noisy because they covary. Since the results are difficult to interpret in numeric form, I plot the elasticity curve implied by the $\gamma$ coefficients in figure \ref{fig:cross_elasticity_euclid}. While the curve is not monotonically decreasing \emph{everywhere}, we do find that it is monotonically decreasing on the range containing the majority of the data. As hypothesized, we see that as a movie becomes more distant, it is less influential on the sales of its competitors. Down the line I will need to compute uncertainty in this curve. Given there is likely negative correlation in errors between $\gamma$ coefficients (e.g. overestimating $\gamma_0$ means we're probably underestimating $\gamma_1$), simply propogating the analytic uncertainty will drastically overstate the true variance. I can get around this by bootstrapping, though given the computational intensity of the model I haven't done this yet. 

Putting some magnitudes on these numbers, suppose Disney were releasing \emph{Frozen} opposite \emph{Pok\'emon: The Movie 2000}--a movie in the top decile of similarity. If Disney were instead facing the horror movie \emph{Premonition}--a movie in the bottom decile of similarity to \emph{Frozen}--they would expect a 0.44\% revenue increase. 

\begin{table}
    \begin{center}
    \input{../elasticity_estimate/output/euclid_regression.tex}

    \caption{Regression Results, Euclidean Distance}
    \label{tab:results_euclid}
    \end{center}
\end{table}

\begin{figure}
    \begin{center}
    \includegraphics[width=\textwidth]{../elasticity_estimate/output/cross_elasticity_euclid.png}
    \caption{Cross-Elasticity by Euclidean Distance}
    \label{fig:cross_elasticity_euclid}
    \end{center}
\end{figure}

Moving now to the specification using cosine distance, the regression results are visible in table \ref{tab:results_cosine}. Again, we see that the results are highly significant. Moreover, here the coefficients on \emph{distance} are also significant, even with the colinearity. The elasticity curve implied by the $\gamma$ coefficients is visible in figure \ref{fig:cross_elasticity}. While no longer monotonically decreasing, we see the pattern is still decreasing on net from the 10th percentile to the 90th percentile of the data.We also see decreasing behavior in the tails, as desired. 

Again putting some concrete meaning on these numbers, suppose Disney were releasing \emph{Frozen} opposite \emph{Inkheart}--a movie in the top decile of cosine similarity. If Disney were instead facing the gory historical action flick \emph{Medieval}--a movie in the bottom decile of cosine similarity to \emph{Frozen}--they would expect a 0.32\% revenue increase. 

\begin{table}
    \begin{center}
    \input{../elasticity_estimate/output/cosine_regression.tex}

    \caption{Regression Results, Cosine Similarity}
    \label{tab:results_cosine}
    \end{center}
\end{table}

\begin{figure}
    \begin{center}
    \includegraphics[width=\textwidth]{../elasticity_estimate/output/cross_elasticity_zoom.png}

    \caption{Cross-Elasticity by Distance}
    \label{fig:cross_elasticity}
    \end{center}
\end{figure}

Note the magnitudes of these elasticity estimates are both quite similar and precisely estimated! Moreover, the $R^2$ of the two regressions is also nearly identical. This suggests that the distance metric we use is not particularly critical for our results.

It is reasonable to ask how well these trends are explained by movie genre alone. When I include a dummy for ``only film of genre'' showing in a given weekend, I find it does not materially change these results. Down the line I will likely want to do something more intensive--e.g., compute how similar genres are, then do something with ``genre distance'' alone rather than using the text information. Even if the genre \emph{can} explain much of the variation, I still believe this method is a contribution in that researchers are more likely to have raw, uncleaned text data than to have properly coded genre data. 


\section{Benchmarking: Collaborative Filtering}

``Recommendation engines'' are a common tool in the tech world, used to predict what products a user will like. In some sense, we can think of this project as harnessing a new type of recommendation engine, identifying movies which are likely to be substitutable. Algorithms in this space are broadly divided into two categories: collaborative filtering and content-based filtering. The text embeddings used above are a form of content-based filtering: using the ``content'' of the movie (its text description) to predict its appeal. However, one way to benchmark the quality of this approach is to compare them to the results of a collaborative filtering algorithm. Unlike content-based filtering, collaborative filtering doesn't include any information about the content of a movie; instead, it uses the preferences of other users to predict what a user will like. For example, if I like \emph{The Dark Knight}, and most people who like \emph{The Dark Knight} also like \emph{Inception}, a collaborative filtering algorithm would predict I would like \emph{Inception}. 

MovieLens produces a public-use movie review dataset; it contains more than 25 million (CHECK THIS NUMBER) 1-to-5 star reviews of movies by users. Moreover, movies are tagged with their TMDB ID; that is, the same ID I use to collect movie descriptions. Thus, for any given pair of movies, I can compute similarity under both the text-based and collaborative filtering methods. If the two measures are correlated, I can be more confident that the patterns I see in the text-based model are not coincidental.

\subsection{UV Decomposition}

For my collaborative filtering algorithm, I use UV-decomposition for matrix completion as described in Leskovec (CITE HERE). Suppose I have an $n \times m$ matrix of movie reviews, with $n$ users and $m$ reviews. However, the matrix is generally sparse; most users have not reviewed most movies. UV-decomposition decomposes this matrix into two thin matrices $U$ and $V$ such that $UV^T$ approximates the original matrix. This is a cousin of singular value decomposition, however relaxes the orthogonality constraint common in those approaches. By minimizing the sum of squared errors among projections in the \emph{observed} entries of the matrix, we hope to project the \emph{unobserved} entries of the matrix as well.

Note that UV decomposition relies on a contraction mapping: for a fixed $V$ matrix, we choose the $U$ matrix which minimizes the sum of squared errors. We then fix that $U$ matrix and choose an optimal $V$ matrix, iterating this process until convergence. This procedure is guaranteed to converge, though it may converge to a local minimum. Thus, we generally initialize the $U$ and $V$ matrices with random values, then run the algorithm multiple times to ensure we find something approximating the global minimum. 

With our decomposition in hand, it is simple to assess whether two movies are ``similar:'' we simply compute the cosine similarity of the projected reviews. If two movies have similar projected reviews, we can infer they appeal to similar audiences, and so are likely substitutable. The virtue of UV-decomposition is it works \emph{even when the movies have no reviewers in common}; that is, it can infer similarity between movies which have never been reviewed by the same person.

\subsection{Validation}

Rather than blindly trusting our decomposition, we can validate it by comparing predicted reviews against actual reviews. I hold out 5\% of the reviews as a test set, then train on the remaining 95\%. I then predict the held-out reviews using the trained UV decomposition and compare the predicted reviews to the actual reviews. If the predicted reviews are highly correlated with the actual reviews, we can be more confident in the UV decomposition.

The relationship between predicted and actual reviews is visible in figure FIGURE. As you can see, there exists a strong positive correlation between the predicted reviews and the actual reviews. The strength of this correlation varies based on the number of reviews a movie has received in the data. Points are colored by the number of reviews their movie received; as is visible in the figure, movies with few reviews have a quite weak correlation between predicted and actual reviews. However, once we trim off the least popular 5\% of movies, the correlation between predicted and actual reviews becomes quite a bit stronger. This makes sense: our model is (as expected) better at predicting a movie's appeal when we have more data on that movie.

These results affirm for me that our collaborative filtering results are likely to be quite predictive of a movie's appeal. Thus, if my text-based model identifies the same pairs of movies as substitutable, I can be more confident that the patterns I see reflect consumers' true preferences. 

\subsection{Comparison}

LOREM IPSUM


\section{Conclusion}

Based on my findings this quarter, I am increasingly confident there is ``something'' here: using boilerplate embeddings of unstructured text characteristics, I pick up significant substitution patterns in the movie industry. If my work is successful, I would be the first to estimate cross-movie elasticities, while also providing a new method for estimating demand in industries with unstructured product characteristic text.

Over the summer, I hope to develop my paper in a few specific ways:
\begin{itemize}
    \item Enrich the model by interacting competitor age with cross-elasticity as discussed in section 4.2
    \item Shore up arguments about substitution by comparing text-based similarity to review-based similarity as discussed in section 8
    \item Confirm in the data that a movie's characteristics appear to be uncorrelated with the characteristics of its competitors as discussed in section 3.1
\end{itemize}

On a broader timeline, I also want to think about some way to address marketing. While the characteristics of the movies themselves may be set in advance, the characteristics \emph{emphasized in marketing} may be different. For example, if two romantic comedies come out in one weekend, both may benefit if one emphasizes the romantic half of the movie while the other emphasizes the comedic half. I am still thinking about clever ways to address this. TMDB contains some movie trailers, so one approach would be to see if similar \emph{movies} also have similar \emph{trailers}, even if they're released at roughly the same time. 

\printbibliography

\end{document}